经过前面的知识获取阶段，我们已经拥有了大量的与经济责任审计领域相关的语料库。这些杂乱无章的语料让我们自己从中获取信息都是一件极其困难的事，更不用说让程序去理解它们了，所以我们需要将这些非结构化的数据转换为结构化的数据，既方便数据库的存储，也方便程序内部使用。

前文也已经提到过，知识图谱就是一个大型的知识网络，组成这个网络的主要元素就是实体和实体之间的关系，那么我们的目标就很清晰了，就是要从这一篇篇文章中提取出实体和实体之间的关系。

在我们正在构建的经济责任审计知识图谱中，实体就是该领域中的名词，比如：“经济责任审计”、“离任审计”等等词汇。现在业界已经开发出很多不同的自然语言处理包，它们都能识别短的词汇，比如“经济责任审计”一般会被自然语言处理包识别为“经济”、“责任”、“审计”三个词汇，而我们希望的是程序将它们当成一个完整的词汇。

业界不同企业、科研机构所实现的自然语言处理包都是适应大众场景的，所以才会出现把“经济责任审计”分为三个词的情况，而经济责任审计领域是一个特定的场景，所以为了达到上述目标，我们需要自己构建用户词典，让程序优先按照我们所给定的分词规则去分词。

实现程序自动化的实体关系抽取是自然语言处理领域一直以来的研究热点，也是是业界一直以来没有解决的难点，因为其面向的是开放的文本，毫无规则，所以抽取难度极大。目前常用的实体关系抽取方法主要有三种：

（1）人工抽取，人工抽取是目前最可靠的方式，自然语言本身就是人类发明的，人类可以很轻松的理解文本数据，但是机器无法理解，所以人工抽取的事实三元组是目前认为最可靠的。但是其缺点也很明显，需要大量的人力资源支撑，同时也耗时耗力，效率底下是最大的缺点。

（2）基于深度学习的实体关系抽取，这种方法目前是既高效又相对可靠的抽取方式，它通过大量的数据进行训练，最后用训练所得的模型抽取实体关系，在效率上得到了极大的提高，同时所提取的实体关系质量也能和人工媲美。这种方式最大的问题是训练集的标注，对训练集的要求很高，而且需要大量的训练集才能得到好的效果。训练集的标注是症结所在，所以本文没有采用这种方法。

（3）[基于依存句法分析的实体关系抽取](http://www.docin.com/p-1715877509.html)，这种方法是一种基于规则的抽取方法，首先把句子分析成一棵依存句法树，然后再通过规则遍历这棵依存句法树得到实体关系。而且对于依存句法分析的研究现在已经比较成熟，本文直接采用开源汉语言处理包HanLP提供的依存句法分析接口得到依存句法树。

Zhang等人<sup>文献 1</sup>采用了基于实例的无监督学习方法，所得的实验效果也比较好，本文参考了[《基于依存分析的开放式中文实体关系抽取方法》](http://www.docin.com/p-1715877509.html)(读者一定要先把这篇论文读读)所提供的方法，使用 Java 语言实现了实体关系抽取程序，并参考文献 2 所提出的方法，实现了非动词性关系的抽取。相比较第（1）、（2）种方法，基于依存句法分析的方法不需要大量的训练集标注，也不需要大量的人力资源，是我们唯一可以选择的方案。


# 依存句法分析

句法分析是自然语言处理中的关键技术之一，它是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析是语言理解的重要一环，同时它也为其它自然语言处理任务提供支持。在我们所使用的的实体关系抽取方法中，句法分析就是为事实三元组抽取提供支持。

一般根据句法结构的表示形式不同，又会将句法分析任务拆分为句法结构分析、依存句法分析、深层文法句法分析。句法结构分析的作用是识别出句子中的短语以及短语之间的层次句法关系；依存分析的作用是识别句子中词汇与词汇之间的相互依存关系；深层文法句法分析是对句子深层的句法以及予以语义进行分析。

依存句法认为“谓语”中的动词是一个句子的中心，其他成分直接或间接的与之产生联系，这也就是我们所抽取出的关系都是谓语动词的原因，学过编译原理的同学可以类比编译原理中的句法分析，我们这里所说的依存句法分析也是为了拿到一个由被分析句子所组成的语法树。

比如我们向 HanLP 依存句法分析器输入一个句子“微软公司于1975年由比尔·盖茨和保罗·艾伦创立，18年启动以智慧云端、前端为导向的大改组。” HanLP 会输出下面的结果。

![uRcAmj.png](https://s2.ax1x.com/2019/10/07/uRcAmj.png)

从图中我们可以看出，该句子的各个成分被清晰的标注出来，词汇与词汇之间的关系也被标注出来，我们的工作就是要利用这些标注出来的信息抽取出句子中实体与实体之间的关系。

# 实体关系抽取

我们使用的基于依存句法分析的开放式中文实体关系抽取方法进行事实三元组的抽取，此方法所抽取的关系表述都是基于谓语动词的，主要分三步：分词、依存句法分词、实体关系抽取。关于分词的必要说明前文已经表述过了，让分词工具按照我们所设定的经济责任审计领域的规则去分词。

《基于依存分析的开放式中文实体关系抽取方法》中指出，中文关系表述形式应当为：```状语*动词+补语?宾语?```。并且给出了一个例子，例子指出指出“p4生于山西”应该抽取出（p4，山西，生于山西），但我认为关系不应该表述为“生于山西”，所以我把关系表述改为下面的样子了。

```
中文关系表述：状语*动词+补语?
```

其中 * 表示出现 0 次或任意多次，+ 表示至少出现一次，？表示出现 1 次或 0 次。我对每个句子的依存句法分析树进行了重新组织，针对每一个词语都为其存储一个依存字典，构建句法依存列表的算法描述如下：

```
算法 1：构建依存句法列表
输入：依存句法分析结果parser
输出：词语依存字典
（1）	定义依存句法列表list
（2）	for curword in parser：
（3）	    定义依存句法字典dict
（4）	    for word in parser：
（5）	        if curword是word的依存依存词语：
（6）	            if 字典已经包含word与curword的依存关系rel：
（7）	                将word加入键rel所对应的列表中
（8）	            else：
（9）	                将rel：word对应的键值对加入到依存字典dict中
（10）	        end if
（11）	    end for
（12）	    将依存字典dict加入到依存句法列表list中
（13）	end for
（14）	return 依存句法列表list
```

经过上述算法对依存句法分析结果重新进行组织之后，使得后面的实体关系抽取算法更加容易设计。因为实体关系抽取的情况较多，且每种情况的抽取形式又大致相似，所以此处只用主谓宾做简单举例说明，主谓宾关系抽取算法描述如下：

```
算法 2：主谓宾关系抽取算法
输入：句法依存字典dict
输出：事实三元组
（1）	if dict中同时包含键值“主谓关系”和“动宾关系”：
（2）	    定义关系rel为dict所属词语
（3）	    定义entity1为dict中键值为“主谓关系”的词语
（4）	    定义entity2为dict中键值为“动宾关系”的词语
（5）	    return entity1, rel, entity2
（6）	end if
```

现在理论分析已经全部完成了，下面我们就一步一步来进行将上述理论用实际代码来实现，下面是主要代码。

```java
    // ParserUtil.java

    /**
     * @param text 待分析的句子
     * @return 分析结果，按分词结果的顺序组织的
     */
    public static List<CoNLLWord> parser(String text){

        CoNLLSentence sentence = HanLP.parseDependency(text);

        CoNLLWord wordArr[] = sentence.getWordArray();

        List<CoNLLWord> result = new ArrayList<>();

        for (int i = 0; i < wordArr.length; i++) {
            result.add(wordArr[i]);
        }

        return result;
    }

    /**
     * 最外层list是为了记录是第几个
     * map中的key记录的是关系
     * map中的list记录的是这个关系的词语
     * @param text 带分析的句子
     * @return 词语依存字典
     */
    public static List<Map<String, List<CoNLLWord>>> dict(String text){

        CoNLLSentence sentence = HanLP.parseDependency(text);

        //System.out.println(sentence);

        CoNLLWord[] wordArray = sentence.getWordArray();

        List<Map<String, List<CoNLLWord>>> result = new ArrayList<>();

        for (int i = 0; i < wordArray.length; i++) {

            CoNLLWord word = wordArray[i];
            HashMap<String, List<CoNLLWord>> map = new HashMap<>();

            for (int j = 0; j < wordArray.length; j++) {
                CoNLLWord child = wordArray[j];
                if (word.LEMMA.equals(child.HEAD.LEMMA)){

                    if (map.containsKey(child.DEPREL)){
                        map.get(child.DEPREL).add(child);
                    }else {
                        List<CoNLLWord> list= new ArrayList<>();
                        list.add(child);

                        map.put(child.DEPREL, list);
                    }

                }
            }

            result.add(map);
        }

        return result;
    }
```
上面的程序主要是为了将句子转换为另一种结构，比如我们的输入的句子是“刘小绪生于四川”，那么当我们分别调用分词接口和依存句法分析接口时，HanLP 将会给出如下结果。

```python
原文：刘小绪生于四川

# 这是分词结果
[刘小绪/nr, 生于/v, 四川/ns]

#这是句法分析结果
刘小绪 --(主谓关系)--> 生于
生于 --(核心关系)--> ##核心##
四川 --(动宾关系)--> 生于

```

为了方便理解，也为了方便程序的编写，我把他们组织成了下面的形式，为每一个词语都建一个依存句法字典。

```python
刘小绪：{}
生于：{主谓关系=[刘小绪], 动宾关系=[四川]}
四川：{}
```
有了上面的基础，我们只需要按照不同的情况对句子分别编写相应的处理程序即可，下面是我所实现的关系抽取程序。

```java
    // RelationUtil.java

    /**
     * @param parser 句法依存分析
     * @param dict 词语依存字典
     * @param i 词语索引
     * @return 三元组列表
     */
    private static Set<String> extract(List<CoNLLWord> parser,
                                       List<Map<String, List<CoNLLWord>>> dict,
                                       int i) {
        CoNLLWord word = parser.get(i);
        Map<String, List<CoNLLWord>> dic = dict.get(i);

        Set<String> result = new HashSet<>();

        // 主谓宾关系：刘小绪生于四川
        if (dic.containsKey("主谓关系") && dic.containsKey("动宾关系")){

            CoNLLWord entity1 = dic.get("主谓关系").get(0);

            // 排除：刘小绪和李华是朋友
            // entity1.ID-1 即主语在依存字典中的索引
            if (dict.get(entity1.ID-1).containsKey("并列关系")){
                String relation = dic.get("动宾关系").get(0).LEMMA;

                CoNLLWord entity2 = dict.get(entity1.ID-1).get("并列关系").get(0);

                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }else {
                CoNLLWord entity2 = dic.get("动宾关系").get(0);

                String relation = word.LEMMA;

                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }
        }

        // 动补结构：刘小绪洗干净了衣服
        if (dic.containsKey("动补结构") && dic.containsKey("主谓关系") && dic.containsKey("动宾关系")){

            CoNLLWord entity1 = dic.get("主谓关系").get(0);
            CoNLLWord complement = dic.get("动补结构").get(0);
            CoNLLWord entity2 = dic.get("动宾关系").get(0);

            if (dic.containsKey("右附加关系")){
                CoNLLWord subjoin = dic.get("右附加关系").get(0);
                String relation = word.LEMMA + complement.LEMMA + subjoin.LEMMA;
                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }else {
                String relation = word.LEMMA + complement.LEMMA;
                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }
        }

        if (dic.containsKey("定中关系")){
            CoNLLWord entity1 = dic.get("定中关系").get(0);
            String relation = word.LEMMA;

            for (Map<String, List<CoNLLWord>> tempDic:
                 dict) {
                if (tempDic.containsKey("主谓关系") && tempDic.containsKey("动宾关系")){
                    if (tempDic.get("主谓关系").get(0).LEMMA.equals(relation)){
                        CoNLLWord entity2 = tempDic.get("动宾关系").get(0);
                        result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
                    }
                }
            }
        }

        // 状动结构：父亲非常喜欢跑步
        // 非常 是 跑步的状语，关系应该为非常喜欢
        if (dic.containsKey("状中结构") && dic.containsKey("主谓关系") && dic.containsKey("动宾关系")){

            CoNLLWord entity1 = dic.get("主谓关系").get(0);
            CoNLLWord adverbial = dic.get("状中结构").get(0);
            CoNLLWord entity2 = dic.get("动宾关系").get(0);

            String relation = adverbial.LEMMA + word.LEMMA;

            result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
        }


        // 状动补结构：
        if (dic.containsKey("状中结构") && dic.containsKey("动补结构") &&
                dic.containsKey("主谓关系") && dic.containsKey("动宾关系")){

            CoNLLWord entity1 = dic.get("主谓关系").get(0);
            CoNLLWord adverbial = dic.get("状中结构").get(0);
            CoNLLWord complement = dic.get("动补结构").get(0);
            CoNLLWord entity2 = dic.get("动宾关系").get(0);

            String relation = adverbial.LEMMA + word.LEMMA + complement.LEMMA;

            result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
        }


        // 定语后置：父亲是来自肯尼亚的留学生
        if (word.DEPREL.equals("定中关系")){
            if (dic.containsKey("动宾关系")){
                CoNLLWord entity1 = word.HEAD;
                String relation = word.LEMMA;
                CoNLLWord entity2 = dic.get("动宾关系").get(0);

                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }
        }

        // 介宾关系：刘小绪就职于学校
        // 于 和 学校 是介宾关系
        if (dic.containsKey("主谓关系") && dic.containsKey("动补结构")){

            CoNLLWord entity1 = dic.get("主谓关系").get(0);
            CoNLLWord prep = dic.get("动补结构").get(0);

            // 介词的索引
            int prepIndex = prep.ID - 1;

            Map<String, List<CoNLLWord>> prepDict = dict.get(prepIndex);
            if (prepDict.containsKey("介宾关系")){
                CoNLLWord entity2 = prepDict.get("介宾关系").get(0);
                String relation = word.LEMMA + prep.LEMMA;

                result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
            }
        }

        // 宾语前置结构：海洋由水组成
        if (dic.containsKey("前置宾语")){

            CoNLLWord entity2 = dic.get("前置宾语").get(0);
            if (dic.containsKey("状中结构")){
                CoNLLWord adverbial = dic.get("状中结构").get(0);
                int prepIndex = adverbial.ID - 1;
                Map<String, List<CoNLLWord>> prepDict = dict.get(prepIndex);

                if (prepDict.containsKey("介宾关系")){
                    CoNLLWord entity1 = prepDict.get("介宾关系").get(0);

                    String relation = word.LEMMA;
                    result.add(entity1.LEMMA + "," + relation + "," + entity2.LEMMA);
                }
            }
        }
        return result;
    }
```

经过测试，上述程序在句子很短的情况下表现还不错，在长句子上面表现的很差劲，下面是一个简单的测试效果。

```python
输入：刘小绪非常喜欢跑步
    > 刘小绪,喜欢,跑步
    > 刘小绪,非常喜欢,跑步

输入：刘小绪和李华是朋友
    > 刘小绪,朋友,李华

输入：刘小绪生于四川
    > 刘小绪,生于,四川

输入：刘小绪洗干净了衣服
    > 刘小绪,洗,衣服
    > 刘小绪,洗干净了,衣服

输入：海洋由水组成
    > 水,组成,海洋

输入：父亲是来自肯尼亚的留学生
    > 父亲,是,留学生

输入：刘小绪就职于学校
    > 刘小绪,就职于,学校

输入：中国的首都是北京
    > 中国,首都,北京
    > 首都,是,北京
```

# 合理利用已有的成果

本书开篇及本章前文已经提及过，实体关系抽取是自然语言处理领域的一大难点，所有有一些公司使用了大量的人力去抽取，人工抽取的关系其准确度自然相当高，远远胜出我们上面程序所达到的效果，所以即使人工抽取的关系很少，我们也应该合理的将这部分优质数据利用起来。Wikidata 就提供了大量的相关数据，这里直接利用网络爬虫将其抓取下来即可。

```python
import requests
from bs4 import BeautifulSoup

base_url = 'https://www.wikidata.org/wiki/'


# 获取关系列表
def get_html(entity, entity_id, entity_en):
    res = requests.get('https://www.wikidata.org/wiki/' + entity_id)
    res.encoding = 'utf-8'
    soup = BeautifulSoup(res.text, 'html.parser')
    list_view = soup.select('.wikibase-listview')
    print(entity, '       ', entity_en)
    if len(list_view) is not 0:
        rel_list = list_view[0].select('.wikibase-statementgroupview')
        for rel in rel_list:
            # 关系和属性混在一起，第一个一定是关系，后面不一定
            a_list = rel.select('a')
            # print(a_list)
            process_a_list(entity, entity_id, entity_en, a_list)


# 选择关系
def process_a_list(entity, entity_id, entity_en, a_list):
    if len(a_list) is not 0:
        # 关系名称
        rel_name = a_list[0].text
        # 关系id
        rel_url = a_list[0]['href']

        if rel_name != 'Commons category' and rel_name != "topic's main category":
            head_str = '"' + entity + '","' + entity_id + '","' + entity_en + '","' + rel_name + '","' + rel_url + '",'
            process_relation(head_str, a_list)


# 处理关系
def process_relation(head_str, a_list):
    end = len(a_list)
    i = 1
    while i < end:
        try:
            # 如果长度为1，说明是实体
            write_str = ''
            if len(a_list[i]['title'].split(':')) == 1:
                write_str += head_str + '"' + a_list[i].text + '","' + a_list[i]['title'] + '"'
            else:
                write_str += '"' + a_list[i - 1].text + '","' + a_list[i - 1]['title'] + '","' \
                             + a_list[i].text + '","' + a_list[i]['href'] + '",' \
                             + '"' + a_list[i + 1].text + '","' + a_list[i + 1]['title'] + '"'
                i += 1
            write_file('实体关系', write_str)
            i += 1
        except Exception as e:
            i += 1
            # print(e, ',无title......')


# 写入文件
def write_file(file_name, content):
    fp = open(file_name + '.txt', 'a', encoding='utf-8')
    try:
        fp.write(content + '\n')
    finally:
        fp.close()


# 从文件中读取数据，对每一行进行处理
def read_file(file_name):
    fp = open(file_name + '.json', 'r', encoding='utf-8')
    try:
        for line in fp:
            dic = eval(line)
            get_html(dic['name'], dic['id'], dic['en_name'])
    finally:
        fp.close()
```

上面的代码即是从 Wikidata 中抓取实体关系的程序，您可以选择使用 Scrapy 实现一个更高质量的程序。到此，实体关系抽取就告一段了，我们也假装自己通过自己的方式暂时解决了一个业界的难题吧。